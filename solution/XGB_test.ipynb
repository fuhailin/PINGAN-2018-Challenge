{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../dump/train_modified.csv')\n",
    "test_df = pd.read_csv('../dump/test_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    697707\n",
       "1.0      3258\n",
       "Name: acc_now_delinq, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target='acc_now_delinq'\n",
    "IDcol = 'member_id'\n",
    "train_df[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [x for x in train_df.columns if x not in [target,IDcol]]\n",
    "params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eta': 0.1,\n",
    "        'max_depth': 9,\n",
    "        'seed': 27,\n",
    "        'n_estimators':1000,\n",
    "        'min_child_weight':1,\n",
    "        'gamma':0,\n",
    "        'subsample':0.8,\n",
    "        'colsample_bytree':0.8,\n",
    "        'n_jobs' : -1,\n",
    "        'scale_pos_weight':100,\n",
    "#     'eval_metric' : 'auc'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_error(preds,dtrain): #preds是结果（概率值），dtrain是个带label的DMatrix\n",
    "    beta=2\n",
    "    label=dtrain.get_label() #提取label\n",
    "    \n",
    "    preds = 1.0/(1.0 + np.exp(-preds))\n",
    "    pred = [int(i>=0.5) for i in preds]\n",
    "    \n",
    "    tp=sum([int(i==1 and j==1) for i,j in zip(pred,label)])\n",
    "    precision = float(tp)/sum(pred)\n",
    "    recall=float(tp)/sum(label)\n",
    "    \n",
    "    return 'f1-score', -1*(precision*recall/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxRecall(preds,dtrain): #preds是结果（概率值），dtrain是个带label的DMatrix\n",
    "    labels=dtrain.get_label() #提取label\n",
    "    preds=1-preds\n",
    "    precision,recall,threshold = metrics.precision_recall_curve(labels,preds,pos_label=0)\n",
    "    pr=pd.DataFrame({'precision':precision,'recall':recall})\n",
    "    return 'Max Recall:',pr[pr.precision>=0.5].recall.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_error(preds,dtrain):\n",
    "    beta=2\n",
    "    label=dtrain.get_label() #提取label\n",
    "    \n",
    "    preds = 1.0/(1.0 + np.exp(-preds))\n",
    "    pred = [int(i>=0.5) for i in preds]\n",
    "    \n",
    "    tp=sum([int(i==1 and j==1) for i,j in zip(pred,label)])\n",
    "    precision = float(tp)/sum(pred)\n",
    "    recall=float(tp)/sum(label)\n",
    "    \n",
    "    return 'f1-score', -1*(precision*recall/(precision+recall))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split Train/Test data\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_df[predictors], train_df[target], test_size=0.15, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.154731\ttest-error:0.154731\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "Multiple eval metrics have been passed: 'test-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until test-f1-score hasn't improved in 15 rounds.\n",
      "[1]\ttrain-error:0.101992\ttest-error:0.101992\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[2]\ttrain-error:0.089529\ttest-error:0.089529\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[3]\ttrain-error:0.082404\ttest-error:0.082404\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[4]\ttrain-error:0.07991\ttest-error:0.07991\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[5]\ttrain-error:0.077546\ttest-error:0.077546\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[6]\ttrain-error:0.074071\ttest-error:0.074071\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[7]\ttrain-error:0.073648\ttest-error:0.073648\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[8]\ttrain-error:0.073061\ttest-error:0.073061\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[9]\ttrain-error:0.072118\ttest-error:0.072118\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[10]\ttrain-error:0.071145\ttest-error:0.071145\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[11]\ttrain-error:0.071283\ttest-error:0.071283\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[12]\ttrain-error:0.07047\ttest-error:0.07047\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[13]\ttrain-error:0.070252\ttest-error:0.070252\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[14]\ttrain-error:0.06934\ttest-error:0.06934\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "[15]\ttrain-error:0.068705\ttest-error:0.068705\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-error:0.154731\ttest-error:0.154731\ttrain-f1-score:-0.004626\ttest-f1-score:-0.004626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgbtrain = xgb.DMatrix(train_df[predictors], train_df[target])\n",
    "xgbvalid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "xgbtest = xgb.DMatrix(test_df[predictors])\n",
    "watchlist = [(xgbtrain, 'train'), (xgbtrain, 'test')]\n",
    "num_rounds = 60\n",
    "model = xgb.train(params, xgbtrain, num_rounds, watchlist, early_stopping_rounds=15\n",
    "                  , feval = f1_error\n",
    "                 )\n",
    "p_valid = model.predict(xgbvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Booster' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-9bd7151792f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Predict training set:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdtrain_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgbvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdtrain_predprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgbvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Booster' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "#Predict training set:\n",
    "dtrain_predictions = model.predict(xgbvalid)\n",
    "dtrain_predprob = model.predict_proba(xgbvalid)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Print model report:\n",
    "print(\"\\nModel Report\")\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(train_df['acc_now_delinq'].values, dtrain_predictions))\n",
    "print(\"Recall : %.4g\" % metrics.recall_score(train_df['acc_now_delinq'].values, dtrain_predictions))\n",
    "print(\"Fbeta Score : %.4g\" % metrics.fbeta_score(train_df['acc_now_delinq'].values, dtrain_predictions,beta=2))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_df['acc_now_delinq'], dtrain_predprob))\n",
    "print(metrics.classification_report(train_df['acc_now_delinq'], dtrain_predictions))\n",
    "\n",
    "#     Predict on testing data:\n",
    "test_df['predprob'] = xgb1.predict_proba(test_df[predictors])[:,1]\n",
    "#     results = test_results.merge(dtest[['ID','predprob']], on='ID')\n",
    "#     print('AUC Score (Test): %f' % metrics.roc_auc_score(results['Disbursed'], results['predprob']))\n",
    "\n",
    "feat_imp = pd.Series(xgb1.get_booster().get_score(importance_type='weight')).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
